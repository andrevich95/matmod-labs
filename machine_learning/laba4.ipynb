{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bade63bcbb4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mtest_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtest_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-bade63bcbb4b>\u001b[0m in \u001b[0;36mtest_mnist\u001b[0;34m(n_examples, num_hidden, epochs, learn_rate)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m    \u001b[0;31m# train one layer of RBM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m    \u001b[0;31m# save all weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-bade63bcbb4b>\u001b[0m in \u001b[0;36mrbm\u001b[0;34m(dataset, num_hidden, learn_rate, epochs, batchsize)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnum_visible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     print(\"Training RBM with\", num_visible, \"visible units,\", \n\u001b[1;32m      5\u001b[0m           \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden units,\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"examples and\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def rbm(dataset, num_hidden, learn_rate, epochs, batchsize):\n",
    "    num_visible = dataset.shape[1]\n",
    "    num_examples = dataset.shape[0]\n",
    "    print(\"Training RBM with\", num_visible, \"visible units,\", \n",
    "          num_hidden, \"hidden units,\", num_examples, \"examples and\", \n",
    "          epochs, \"epochs...\")\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    batches = num_examples // batchsize\n",
    "   \n",
    "    w = 0.1 * np.random.randn(num_visible, num_hidden)\n",
    "    a = np.zeros((1, num_visible))\n",
    "    b = -4.0 * np.ones((1, num_hidden))\n",
    "\n",
    "    w_inc = np.zeros((num_visible, num_hidden))\n",
    "    a_inc = np.zeros((1, num_visible))\n",
    "    b_inc = np.zeros((1, num_hidden))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for batch in range(batches):\n",
    "         #### --- Positive phase of contrastive divergence --- ####\n",
    "\n",
    "         # get next batch of data\n",
    "            v0 = dataset[int(batch*batchsize):int((batch+1)*batchsize)]\n",
    "\n",
    "         # in this matrix, m[i,j] is prob h[j] = 1 given example v[i]\n",
    "         # dims: [num_ex] x [num_hidden]\n",
    "            prob_h0 = logistic(v0, w, b)\n",
    "    \n",
    "         # sample the states of hidden units based on prob_h0\n",
    "            h0 = prob_h0 > np.random.rand(batchsize, num_hidden)\n",
    "\n",
    "         # positive phase products\n",
    "            vh0 = np.dot(v0.T, prob_h0)\n",
    "\n",
    "         # activation values needed to update biases\n",
    "            poshidact = np.sum(prob_h0, axis=0)\n",
    "            posvisact = np.sum(v0, axis=0)\n",
    "\n",
    "         #### --- Negative phase of contrastive divergence --- ####\n",
    "\n",
    "         # reconstruct the data by sampling the visible states from hidden states\n",
    "            v1 = logistic(h0, w.T, a)\n",
    "\n",
    "         # sample hidden states from visible states\n",
    "            prob_h1 = logistic(v1, w, b)\n",
    "\n",
    "         #negative phase products\n",
    "            vh1 = np.dot(v1.T, prob_h1)\n",
    "\n",
    "         # activation values needed to update biases\n",
    "            neghidact = np.sum(prob_h1, axis=0)\n",
    "            negvisact = np.sum(v1, axis=0)\n",
    "\n",
    "         #### --- Updating the weights --- ####\n",
    "\n",
    "         # set momentum as per Hinton's practical guide to training RBMs\n",
    "            m = 0.5 if epoch > 5 else 0.9\n",
    "\n",
    "         # update the weights\n",
    "            w_inc = w_inc * m + (learn_rate/batchsize) * (vh0 - vh1)\n",
    "            a_inc = a_inc * m + (learn_rate/batchsize) * (posvisact - negvisact)\n",
    "            b_inc = b_inc * m + (learn_rate/batchsize) * (poshidact - neghidact)\n",
    "            \n",
    "            a += a_inc\n",
    "            b += b_inc\n",
    "            w += w_inc\n",
    "\n",
    "            error += np.sum((v0 - v1) ** 2)\n",
    "\n",
    "        print(\"Epoch %s completed. Reconstruction error is %0.2f. Time elapsed (sec): %0.2f\" \n",
    "            % (epoch + 1, error, time() - start_time))\n",
    "\n",
    "    print (\"Training completed.\\nTotal training time (sec): %0.2f \\n\" % (time() - start_time))\n",
    "    return w, a, b\n",
    "\n",
    "\n",
    "def logistic(x,w,b):\n",
    "    xw = np.dot(x, w)\n",
    "    replicated_b = np.tile(b, (x.shape[0], 1))\n",
    "\n",
    "    return 1.0 / (1 + np.exp(- xw - b))\n",
    "\n",
    "\n",
    "def reconstruct(v0, w, a, b):\n",
    "    num_hidden = w.shape[1]\n",
    "    prob_h0 = logistic(v0, w, b)\n",
    "    h0 = prob_h0 > np.random.rand(1, num_hidden)\n",
    "\n",
    "    return logistic(h0, w.T, a)\n",
    "\n",
    "def sample_hidden(v0,w,b):\n",
    "    num_hidden = w.shape[1]\n",
    "    return logistic(v0, w, b)\n",
    "\n",
    "\n",
    "def save_weights(w, a, b, directory, n_examples, num_hidden):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    w_name = directory + os.sep + \"w_v\" + str(n_examples) + \"_h\" + str(num_hidden)\n",
    "    np.save(w_name, w)\n",
    "    a_name = directory + os.sep + \"a_v\" + str(n_examples) + \"_h\" + str(num_hidden)\n",
    "    np.save(a_name, a)\n",
    "    b_name = directory + os.sep + \"b_v\" + str(n_examples) + \"_h\" + str(num_hidden)\n",
    "    np.save(b_name, b)\n",
    "\n",
    "\n",
    "def test_mnist(n_examples, num_hidden, epochs, learn_rate):\n",
    "   # load data\n",
    "    images, labels = load_mnist(n_examples, training = True)\n",
    "\n",
    "   # train one layer of RBM\n",
    "    w, a, b  = rbm(images, num_hidden, learn_rate, epochs, batchsize = 100)\n",
    "\n",
    "   # save all weights\n",
    "    print(\"Saving weights...\")\n",
    "    save_weights(w, a, b, \"Output\", n_examples, num_hidden)\n",
    "   \n",
    "   # try to reconstruct some test set images\n",
    "    print(\"Generating and saving the reconstructed images...\")\n",
    "    images, labels = load_mnist(10, training = False)\n",
    "    for i in range(10):\n",
    "        data = images[i]\n",
    "        save_mnist_image(data, \"Output\", str(i) + \"original.png\")\n",
    "        data = reconstruct(data, w, a, b)\n",
    "        save_mnist_image(data, \"Output\", str(i) + \"reconstructed.png\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) == 5:\n",
    "      # user provided the params\n",
    "        n_examples = int(sys.argv[1])\n",
    "        num_hidden = int(sys.argv[2])\n",
    "        epochs = int(sys.argv[3])\n",
    "        learn_rate = float(sys.argv[4])\n",
    "      # run!\n",
    "        test_mnist(n_examples, num_hidden, epochs, learn_rate)\n",
    "    else:\n",
    "        test_mnist(1000, 100, 50, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(n_examples, training = True):\n",
    "    return np.array([[1],[1]])\n",
    "    n_train = n_examples  # The size of training set\n",
    "    # Split dataset into training set (60000) and testing set (10000)\n",
    "    data_train = mnist.data[:n_train]\n",
    "    target_train = mnist.target[:n_train]\n",
    "    data_test = mnist.data[n_train:]\n",
    "    target_test = mnist.target[n_train:]\n",
    "    return (data_train.astype(np.float32), target_train.astype(np.float32),\n",
    "            data_test.astype(np.float32), target_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
